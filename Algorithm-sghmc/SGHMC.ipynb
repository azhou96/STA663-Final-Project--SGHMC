{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_data(data, batch_size, random_seed=45):\n",
    "    \"\"\"\n",
    "    Create minibatch samples from the dataset\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_minibatches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_minibatches)\n",
    "    return(data, n_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sghmc(gradU, eps, C, inv_M, theta_0, V_hat, data, batch_size, burn_in, n_iter=500):\n",
    "    '''\n",
    "    Define SGHMC as described in the paper Stochastic Gradient Hamilton Monte Carlo, \n",
    "    ICML, Beijing, China, 2014 by\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eps = the learning rate\n",
    "    C = user specified friction term\n",
    "    inv_M = inverse of the mass matrix\n",
    "    theta_0 = initial value of parameter sampling\n",
    "    V_hat = estimated covariance matrix using empirical Fisher information\n",
    "    batch_size = size of a minibatch in an iteration\n",
    "    burn_in = number of iterations to drop\n",
    "    n_iter = number of samples to generate\n",
    "\n",
    "    The outpit is:\n",
    "    theta_samples: a np.array of positions of theta.\n",
    "    '''\n",
    "\n",
    "    # parameter vector dimension\n",
    "    p = theta_0.shape[0]\n",
    "    # number of samples\n",
    "    n = data.shape[0]\n",
    "    # placeholder for theta samples\n",
    "    theta_samples = np.zeros((p, n_iter))\n",
    "    theta_samples[:, 0] = theta_0\n",
    "    \n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = (V_hat * eps) / 2\n",
    "    Sigma = np.linalg.cholesky(2 * (C - beta_hat) * eps)\n",
    "    \n",
    "    # data\n",
    "    mini_data, n_batches = minibatch_data(data, batch_size)\n",
    "\n",
    "    # assert batch size to be <= the amount of data\n",
    "    if (batch_size > data.shape[0]): \n",
    "        print(\"Error: batch_size cannot be bigger than the number of samples\")\n",
    "        return\n",
    "    \n",
    "    # loop through algorithm to get n iteration samples\n",
    "    for i in range(n_iter - 1):\n",
    "        theta = theta_samples[:, i]\n",
    "        # resample momentum at each new iteration\n",
    "        M = np.linalg.cholesky(np.linalg.inv(inv_M))\n",
    "        momen = M@np.random.randn(p).reshape(p, -1)\n",
    "        \n",
    "        # sghmc sampler\n",
    "        for j in range(n_batches):\n",
    "            theta = theta + (eps*inv_M@momen).flatten()\n",
    "            gradU_batch = gradU(theta, mini_data[:,:,j], n, batch_size).reshape(p, -1)\n",
    "            momen = momen - eps*gradU_batch - eps*C@inv_M@momen + Sigma@np.random.randn(p).reshape(p, -1)\n",
    "            \n",
    "        theta_samples[:, i+1] = theta\n",
    "        \n",
    "    return theta_samples[:, burn_in:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
