{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import numba\n",
    "from numba import jit\n",
    "from SGHMC.sghmc import sghmc, minibatch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def minibatch_data_numba(data, batch_size, random_seed=45):\n",
    "    \"\"\"\n",
    "    Create minibatch samples from the dataset\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    p = data.shape[1]\n",
    "    if n % batch_size != 0:\n",
    "        n = (n // batch_size) * batch_size\n",
    "    ind = np.arange(n)\n",
    "    np.random.shuffle(ind)\n",
    "    n_minibatches = n // batch_size\n",
    "    data = data[ind].reshape(batch_size, p, n_minibatches)\n",
    "    return(data, n_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba(gradU, eps, C, inv_M, theta_0, V_hat, data, batch_size, burn_in, n_iter=500):\n",
    "    '''\n",
    "    Define SGHMC as described in the paper Stochastic Gradient Hamilton Monte Carlo, \n",
    "    ICML, Beijing, China, 2014 by\n",
    "    Tianqi Chen, Emily B. Fox, Carlos Guestrin.\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eps = the learning rate\n",
    "    C = user specified friction term\n",
    "    inv_M = inverse of the mass matrix\n",
    "    theta_0 = initial value of parameter sampling\n",
    "    V_hat = estimated covariance matrix using empirical Fisher information\n",
    "    batch_size = size of a minibatch in an iteration\n",
    "    burn_in = number of iterations to drop\n",
    "    n_iter = number of samples to generate\n",
    "\n",
    "    The outpit is:\n",
    "    theta_samples: a np.array of positions of theta.\n",
    "    '''\n",
    "\n",
    "    # parameter vector dimension\n",
    "    p = theta_0.shape[0]\n",
    "    # number of samples\n",
    "    n = data.shape[0]\n",
    "    # placeholder for theta samples\n",
    "    theta_samples = np.zeros((p, n_iter))\n",
    "    theta_samples[:, 0] = theta_0\n",
    "    \n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = (V_hat * eps) / 2\n",
    "    Sigma = np.linalg.cholesky(2 * (C - beta_hat) * eps)\n",
    "    \n",
    "    # data\n",
    "    mini_data, n_batches = minibatch_data_numba(data, batch_size)\n",
    "\n",
    "    # assert batch size to be <= the amount of data\n",
    "    if (batch_size > data.shape[0]): \n",
    "        print(\"Error: batch_size cannot be bigger than the number of samples\")\n",
    "        return\n",
    "    \n",
    "    # loop through algorithm to get n iteration samples\n",
    "    for i in range(n_iter - 1):\n",
    "        theta = theta_samples[:, i]\n",
    "        # resample momentum at each new iteration\n",
    "        M = np.linalg.cholesky(np.linalg.inv(inv_M))\n",
    "        momen = M@np.random.randn(p).reshape(p, -1)\n",
    "        \n",
    "        # sghmc sampler\n",
    "        for j in range(n_batches):\n",
    "            theta = theta + (eps*inv_M@momen).flatten()\n",
    "            gradU_batch = gradU(theta, mini_data[:,:,j], n, batch_size).reshape(p, -1)\n",
    "            momen = momen - eps*gradU_batch - eps*C@inv_M@momen + Sigma@np.random.randn(p).reshape(p, -1)\n",
    "            \n",
    "        theta_samples[:, i+1] = theta\n",
    "        \n",
    "    return theta_samples[:, burn_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def sghmc_numba2(gradU, eps, C, inv_M, theta_0, V_hat, data, batch_size, burn_in, n_iter=500):\n",
    "    '''\n",
    "    Define SGHMC as described in the paper\n",
    "\n",
    "    The inputs are:\n",
    "    gradU = gradient of U\n",
    "    eps = eps^2 M^(-1)\n",
    "    C = \n",
    "    inv_M = \n",
    "    theta_0 = initial value of parameter sampling\n",
    "    V_hat = \n",
    "    n_iter = number of samples to generate\n",
    "\n",
    "    The outpit is:\n",
    "    A np.array of positions of theta.\n",
    "    '''\n",
    "\n",
    "    # parameter vector dimension\n",
    "    p = theta_0.shape[0]\n",
    "    # number of samples\n",
    "    n = data.shape[0]\n",
    "    # placeholder for theta samples\n",
    "    theta_samples = np.zeros((p, n_iter))\n",
    "    theta_samples[:, 0] = theta_0\n",
    "    \n",
    "    # fix beta_hat as described on pg. 6 of paper\n",
    "    beta_hat = (V_hat * eps) / 2\n",
    "    Sigma = np.linalg.cholesky(2 * (C - beta_hat) * eps)\n",
    "    \n",
    "    # data\n",
    "    mini_data, n_batches = minibatch_data(data, batch_size)\n",
    "\n",
    "    # assert batch size to be <= the amount of data\n",
    "    if (batch_size > data.shape[0]): \n",
    "        print(\"Error: batch_size cannot be bigger than the number of samples\")\n",
    "        return\n",
    "    \n",
    "    # loop through algorithm to get n iteration samples\n",
    "    for i in range(n_iter - 1):\n",
    "        theta = theta_samples[:, i]\n",
    "        # resample momentum at each new iteration\n",
    "        M = np.linalg.cholesky(np.linalg.inv(inv_M))\n",
    "        momen = M@np.random.randn(p).reshape(p, -1)\n",
    "        \n",
    "        # sghmc sampler\n",
    "        for j in range(n_batches):\n",
    "            theta = theta + (eps*inv_M@momen).flatten()\n",
    "            gradU_batch = gradU(theta, mini_data[:,:,j], n, batch_size).reshape(p, -1)\n",
    "            momen = momen - eps*gradU_batch - eps*C@inv_M@momen + Sigma@np.random.randn(p).reshape(p, -1)\n",
    "            \n",
    "        theta_samples[:, i+1] = theta\n",
    "        \n",
    "    return theta_samples[:, burn_in:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(663)\n",
    "n = 200\n",
    "x = np.zeros((200, 1))\n",
    "theta_0 = np.array([0])\n",
    "p = theta_0.shape[0]\n",
    "eps = 0.1\n",
    "C = np.eye(1)\n",
    "V_hat = np.eye(1)*4\n",
    "batch_size = 5\n",
    "n_iter = 200\n",
    "inv_M = np.eye(p)\n",
    "burn_in = 50\n",
    "\n",
    "def noisy_gradU(theta, x, n, batch_size):\n",
    "    '''noisy gradient from paper fig1'''\n",
    "    return -4*theta + 4*theta**3 + np.random.normal(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 ms ± 7.97 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(noisy_gradU, eps, C, inv_M, theta_0, V_hat, x, batch_size, burn_in, n_iter) #no jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 ms ± 23.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(noisy_gradU, eps, C, inv_M, theta_0, V_hat, x, batch_size, burn_in, n_iter) #jit to both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 ms ± 5.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba2(noisy_gradU, eps, C, inv_M, theta_0, V_hat, x, batch_size, burn_in, n_iter) #jit to sghmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import jacobian\n",
    "import autograd.numpy as np\n",
    "# compute log(theta)\n",
    "def log_prior(theta):\n",
    "    return (-1/(2*10))*theta.T@theta\n",
    "\n",
    "# compute log(x\\theta)\n",
    "def log_like(theta, x):\n",
    "    return np.log(0.5 * np.exp(-0.5*(theta[0]-x)**2) + 0.5* np.exp(-0.5*(theta[1]-x)**2))\n",
    "\n",
    "# function of U(theta)\n",
    "def U(theta, x, n, batch_size):\n",
    "    return -log_prior(theta) - (n/batch_size)*sum(log_like(theta, x))\n",
    "\n",
    "gradU = jacobian(U, argnum = 0)\n",
    "\n",
    "np.random.seed(663)\n",
    "n = 200\n",
    "theta_0 = np.array([0,0])\n",
    "p = theta_0.shape[0]\n",
    "theta = np.array([-3,3]).reshape(p,1)\n",
    "x = np.r_[np.random.normal(theta[0], 1, (n,1)),\n",
    "              np.random.normal(theta[1], 1, (n,1))].reshape(-1,1)\n",
    "eps = 0.01\n",
    "C = np.eye(p)\n",
    "V_hat = np.eye(p)\n",
    "batch_size = 80\n",
    "n_iter = 200\n",
    "inv_M = np.eye(p)\n",
    "burn_in = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.63 s ± 171 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc(gradU, eps, C, inv_M, theta_0, V_hat, x, batch_size, burn_in, n_iter) #no jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6 s ± 172 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba(gradU, eps, C, inv_M, theta_0, V_hat, x, batch_size, burn_in, n_iter) #jit to both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.68 s ± 155 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sghmc_numba2(gradU, eps, C, inv_M, theta_0, V_hat, x, batch_size, burn_in, n_iter) #jit to sghmc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
